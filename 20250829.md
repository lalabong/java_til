
# 캐시와 캐싱: 속도를 위한 임시 저장소의 모든 것

## 캐시가 뭘까?

캐시는 간단히 말해서 "자주 쓰는 것들을 가까운 곳에 미리 준비해두는 것"이다. 커피를 매번 원두를 갈아서 내리는 대신, 인스턴트 커피를 준비해두는 것과 비슷하다.

프로그래밍에서 캐시는 데이터나 연산 결과를 빠른 저장 공간에 임시로 보관해, 다음에 똑같은 요청이 오면 바로 응답할 수 있게 하는 기술이다. 데이터베이스까지 가지 않고도 메모리에서 바로 꺼내올 수 있으니까 훨씬 빠르다.

## 왜 캐시를 쓸까?

### 메모리 계층 구조의 현실

컴퓨터의 저장 장치는 피라미드 구조로 되어 있다.

```
       CPU Register (빠름, 작음, 비쌈)
            ↓
         Cache Memory
            ↓
        RAM (적당함)
            ↓
      하드디스크 (느림, 크고, 싸다)
```

위로 갈수록 빠르지만 용량이 작고 비싸다. 아래로 갈수록 느리지만 큰 용량을 저렴하게 쓸 수 있다. 캐시는 이 특성을 활용해서 자주 쓰는 데이터를 빠른 저장소에 복사해둔다.

### 지역성의 원리

컴퓨터 과학에는 "지역성의 원리"라는 중요한 개념이 있다.

- **시간 지역성**: 한번 사용된 데이터는 곧 다시 사용될 가능성이 높다
- **공간 지역성**: 어떤 데이터가 사용되면 그 근처의 데이터도 곧 사용될 가능성이 높다

예를 들어, 사용자가 상품 목록을 조회하면 곧이어 상품 상세 정보도 조회할 가능성이 높다. 이런 패턴을 활용해 캐시 전략을 세울 수 있다.

## 캐시의 기본 동작 원리

### Cache Hit vs Cache Miss

```java
public Product getProduct(Long productId) {
    // 1. 캐시에서 먼저 찾아본다
    Product cached = cache.get("product:" + productId);
    if (cached != null) {
        // Cache Hit! 바로 반환
        return cached;
    }
    
    // 2. 캐시에 없으면 DB에서 조회
    Product product = productRepository.findById(productId);
    
    // 3. DB에서 가져온 데이터를 캐시에 저장
    cache.put("product:" + productId, product, Duration.ofMinutes(10));
    
    return product;
}
```

**Cache Hit**은 찾는 데이터가 캐시에 있는 경우다. 이때는 DB까지 갈 필요 없이 바로 응답할 수 있다.

**Cache Miss**는 찾는 데이터가 캐시에 없는 경우다. 이때는 원본 데이터 소스(DB 등)에서 가져와야 한다.

### Cache Hit Ratio가 핵심

캐시의 효과는 **Cache Hit Ratio**(적중률)로 측정한다. 전체 요청 중에서 캐시에서 바로 응답한 비율이다.

- 90% Hit Ratio면 10개 요청 중 9개를 캐시로 처리
- Hit Ratio가 높을수록 시스템 성능이 좋아짐
- Hit Ratio가 너무 낮으면 캐시 유지 비용만 든다


## 캐시 읽기 전략

### Cache-Aside (Lazy Loading)

애플리케이션이 직접 캐시를 관리하는 방식이다. 가장 일반적으로 쓰인다.

```python
def get_user(user_id):
    # 1. 캐시에서 확인
    user = redis_client.get(f"user:{user_id}")
    if user:
        return json.loads(user)  # Cache Hit
    
    # 2. DB에서 조회 (Cache Miss)
    user = db.query("SELECT * FROM users WHERE id = %s", user_id)
    
    # 3. 캐시에 저장
    redis_client.setex(f"user:{user_id}", 3600, json.dumps(user))
    
    return user
```

**장점:**

- 캐시 서버가 죽어도 애플리케이션이 DB로 직접 갈 수 있어서 안전하다
- 구현이 단순하다
- 필요한 데이터만 캐시에 들어간다

**단점:**

- Cache Miss가 발생하면 3번의 네트워크 호출이 필요하다 (캐시 확인 → DB 조회 → 캐시 저장)


### Read-Through

캐시가 DB 접근을 대신 해주는 방식이다.

```python
# 캐시 계층에서 자동으로 처리
def get_user_read_through(user_id):
    # 캐시가 알아서 DB까지 확인해서 데이터를 가져온다
    return cache_layer.get(f"user:{user_id}")
```

**장점:**

- 애플리케이션 코드가 간단해진다
- 캐시와 DB 간 일관성 관리가 쉽다

**단점:**

- 첫 번째 요청은 항상 느리다 (Cold Start 문제)
- 캐시 서버에 장애가 생기면 전체 시스템이 영향 받는다


## 캐시 쓰기 전략

### Write-Through

데이터를 쓸 때 캐시와 DB에 동시에 저장하는 방식이다.

```python
def save_user(user):
    # 1. 캐시에 저장
    redis_client.setex(f"user:{user.id}", 3600, json.dumps(user))
    
    # 2. DB에도 저장
    db.execute("UPDATE users SET name = %s WHERE id = %s", user.name, user.id)
```

**장점:**

- 캐시와 DB가 항상 동기화된다
- 데이터 일관성이 좋다

**단점:**

- 쓰기 성능이 느리다 (두 곳에 모두 써야 하니까)
- 불필요한 캐시 데이터가 쌓일 수 있다


### Write-Back (Write-Behind)

일단 캐시에만 쓰고, 나중에 배치로 DB에 반영하는 방식이다.

```python
def save_user_write_back(user):
    # 1. 캐시에만 저장하고 dirty 마킹
    cache_layer.set_dirty(f"user:{user.id}", user)
    
    # 2. 별도의 백그라운드 프로세스가 주기적으로 DB에 반영
    # background_sync_job()
```

**장점:**

- 쓰기 성능이 매우 빠르다
- DB 부하를 줄일 수 있다

**단점:**

- 캐시 서버가 죽으면 데이터를 잃을 수 있다
- 구현이 복잡하다


### Write-Around

쓰기 작업은 DB에만 하고, 캐시는 읽을 때만 채우는 방식이다.

```python
def save_user_write_around(user):
    # 1. DB에만 저장
    db.execute("UPDATE users SET name = %s WHERE id = %s", user.name, user.id)
    
    # 2. 캐시에서 해당 데이터 삭제 (무효화)
    redis_client.delete(f"user:{user.id}")
```

**장점:**

- 쓰지 않는 데이터가 캐시를 차지하지 않는다
- 구현이 비교적 단순하다

**단점:**

- 최신 데이터를 읽을 때 Cache Miss가 발생한다


## 캐시 교체 알고리즘

캐시 공간은 한정되어 있으므로, 공간이 부족하면 기존 데이터를 지워야 한다. 어떤 데이터를 지울지 결정하는 것이 교체 알고리즘이다.

### LRU (Least Recently Used)

가장 오랫동안 사용되지 않은 데이터를 먼저 삭제한다. 가장 많이 쓰인다.

```python
class LRUCache:
    def __init__(self, capacity):
        self.capacity = capacity
        self.cache = OrderedDict()
    
    def get(self, key):
        if key in self.cache:
            # 최근 사용으로 갱신
            value = self.cache.pop(key)
            self.cache[key] = value
            return value
        return None
    
    def put(self, key, value):
        if key in self.cache:
            self.cache.pop(key)
        elif len(self.cache) >= self.capacity:
            # 가장 오래된 것 삭제
            self.cache.popitem(last=False)
        
        self.cache[key] = value
```


### FIFO (First In First Out)

먼저 들어온 데이터를 먼저 삭제한다. 구현이 간단하지만 효율성은 떨어진다.[

### TTL (Time To Live)

일정 시간이 지나면 자동으로 삭제되는 방식이다.

```python
# Redis 예시
redis_client.setex("user:123", 300, user_data)  # 5분 후 자동 삭제
```


## 실제 서비스에서 캐시 활용

### 웹 서비스의 계층별 캐시

```
브라우저 캐시 → CDN → 로드밸런서 → 앱 서버 캐시 → DB 캐시
```

각 계층마다 다른 캐시 전략을 적용한다.

### Database Query 캐시

```java
@Cacheable(value = "products", key = "#categoryId")
public List<Product> getProductsByCategory(Long categoryId) {
    return productRepository.findByCategoryId(categoryId);
}

@CacheEvict(value = "products", key = "#product.categoryId")
public void saveProduct(Product product) {
    productRepository.save(product);
}
```

Spring의 `@Cacheable`을 사용하면 메서드 결과를 자동으로 캐싱할 수 있다.

### Session 캐시

```python
# Flask + Redis 예시
@app.route('/login', methods=['POST'])
def login():
    user = authenticate(request.json['username'], request.json['password'])
    session_id = generate_session_id()
    
    # 세션 정보를 Redis에 저장
    redis_client.setex(f"session:{session_id}", 1800, json.dumps({
        'user_id': user.id,
        'username': user.username,
        'login_time': time.time()
    }))
    
    return {'session_id': session_id}
```


### API Response 캐시

```javascript
// Express.js + Redis 예시
app.get('/api/popular-products', async (req, res) => {
    const cached = await redis.get('popular-products');
    if (cached) {
        return res.json(JSON.parse(cached));
    }
    
    const products = await getPopularProducts();
    await redis.setex('popular-products', 300, JSON.stringify(products)); // 5분 캐시
    
    res.json(products);
});
```


## 캐시 사용 시 주의사항

### Cache Stampede 문제

인기 있는 데이터의 캐시가 만료되는 순간, 동시에 많은 요청이 DB로 몰리는 현상이다.

```python
import threading

cache_lock = threading.Lock()

def get_popular_data():
    data = cache.get('popular_data')
    if data:
        return data
    
    # 락을 사용해서 한 번만 DB 조회하도록
    with cache_lock:
        data = cache.get('popular_data')  # 다시 한 번 확인
        if data:
            return data
        
        data = expensive_db_query()
        cache.set('popular_data', data, 300)
        return data
```


### 캐시 일관성 문제

여러 서버에서 같은 캐시를 공유할 때 데이터 불일치가 발생할 수 있다.

```python
# 분산 락을 사용한 해결책
def update_user_with_lock(user_id, new_data):
    lock_key = f"lock:user:{user_id}"
    
    # 분산 락 획득 시도 (5초 타임아웃)
    if redis_client.set(lock_key, "locked", nx=True, ex=5):
        try:
            # 캐시 무효화
            redis_client.delete(f"user:{user_id}")
            
            # DB 업데이트
            db.execute("UPDATE users SET ... WHERE id = %s", user_id)
            
        finally:
            redis_client.delete(lock_key)
    else:
        raise Exception("다른 프로세스가 처리 중입니다")
```


### Hotspot 문제

특정 캐시 키에 요청이 집중되면 해당 캐시 서버에 부하가 몰린다.

```python
# 데이터를 여러 키로 분산 저장
def get_trending_products():
    shard_key = hash(time.time() // 300) % 3  # 5분마다 샤드 변경
    cache_key = f"trending:shard:{shard_key}"
    
    data = cache.get(cache_key)
    if not data:
        data = expensive_trending_calculation()
        cache.set(cache_key, data, 300)
    
    return data
```


## 캐시 모니터링과 성능 측정

### 중요한 메트릭들

```python
class CacheMetrics:
    def __init__(self):
        self.hit_count = 0
        self.miss_count = 0
        self.total_requests = 0
    
    def record_hit(self):
        self.hit_count += 1
        self.total_requests += 1
    
    def record_miss(self):
        self.miss_count += 1
        self.total_requests += 1
    
    def get_hit_ratio(self):
        if self.total_requests == 0:
            return 0
        return self.hit_count / self.total_requests
    
    def get_stats(self):
        return {
            'hit_ratio': self.get_hit_ratio(),
            'total_requests': self.total_requests,
            'cache_hits': self.hit_count,
            'cache_misses': self.miss_count
        }
```


### 캐시 워밍

서비스 시작 시 미리 중요한 데이터를 캐시에 로드해두는 것이다.

```python
def warm_up_cache():
    # 인기 상품 데이터 미리 캐싱
    popular_products = db.query("SELECT * FROM products ORDER BY view_count DESC LIMIT 100")
    for product in popular_products:
        cache.set(f"product:{product.id}", product, 3600)
    
    # 카테고리 데이터 미리 캐싱
    categories = db.query("SELECT * FROM categories WHERE active = true")
    cache.set("categories:active", categories, 7200)
```


## 어떤 캐시를 써야 할까?

### 로컬 캐시 vs 분산 캐시

**로컬 캐시 (In-Memory)**

- 애플리케이션 메모리에 직접 저장
- 매우 빠르지만 서버마다 다른 데이터를 가질 수 있음
- Ehcache, Caffeine, 단순 HashMap 등

**분산 캐시**

- 별도의 캐시 서버에 저장
- 여러 서버가 같은 캐시를 공유
- Redis, Memcached 등


### 캐시 선택 기준

```
읽기 빈도가 높고, 데이터 변경이 적으면 → 캐시 효과 큰
데이터가 작고, 네트워크 비용이 비싸면 → 로컬 캐시
데이터 일관성이 중요하면 → 분산 캐시
서버 메모리가 부족하면 → 외부 캐시 서버
```

캐시는 "더 빠르게" 만들어주는 마법의 도구가 아니다. 잘못 쓰면 오히려 복잡성만 늘어날 수 있다. 하지만 적절한 곳에 적절한 방식으로 쓰면, 사용자 경험과 시스템 효율성을 크게 높일 수 있는 강력한 무기가 된다.

기억할 것은 캐시는 "임시 저장소"라는 점이다. 원본 데이터가 사라져도 괜찮은 것들만 캐시에 넣어야 하고, 항상 원본과의 일관성을 고민해야 한다.
